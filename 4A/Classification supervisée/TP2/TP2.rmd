---
title: "TP2"
author: "Alison Hourdeau"
date: "17/03/2021"
output:
  html_document:
    df_print: paged
---

#1. Evaluation de la règle de classement (iris de Fisher)
##1. Diagnostic apparent 

On charge le TP1 pour avoir tous les résultats précédents.
```{r}
library("knitr")
purl("corrige_TP1.Rmd")
source("corrige_TP1.R")
```


On calcule la matrice de confusion à l'aide de
la fonction table sur la classe réelle et la classe prédite.
```{r}
table(Yreel = iris$Y,Ypredit = Ypredit)
```

On calcule le taux de bon classement (TBC) :
```{r}
TBC=mean(iris$Y==Ypredit)
TBC
```

Puis le taux de mauvais classement (TMC) :
```{r}
TMC = 1 - TBC
TMC
```

La méthode utilisée peut souffrir d'un biais d'optimisme car les données ont servi à apprendre le modèle et à le tester. 
Pour avoir ne pas avoir de biais, on peut découper les données en deux échantillons : un échantillon d'apprentissage (environ 2/3 à 75% de l'échantillon global) et un échantillon test.

##2. Diagnostic sur un échantillon test

On découpe les données iris en un échantillon d'apprentissage (70% des données) et un échantillon test (30% des données) pour construire le tableau de score :
```{r}
n = nrow(iris)
set.seed(1234)

#Tirage
idx=sample(n,round(n*0.7),replace=FALSE)

#Echantillon d'apprentissage
iristrain=iris[idx,]
iristrain
```

```{r}
#Echantillon test
iristest=iris[-idx,]
iristest
```

On fait ensuite la prédiction sur l'échantillon test (30%).
On écrit la fonction nommée calcalpha qui apprend le tableau des coefficients alpha.
```{r}
calcalpha <-function(X, Y){
  d=ncol(X)
  k=nlevels(Y)
  W=matrix(0,d,d)
  ni=table(Y)
  for (i in levels(Y)){
    W=W+cov.wt(X[Y==i,],method="ML")$cov*ni[i]
  }
  W=W/sum(ni)
  moyennes=by(X,Y,colMeans)
  G=matrix(unlist(moyennes),k,d,byrow=T)
  B=cov.wt(G,wt = as.vector(table(Y)),method="ML")$cov
  alpha=matrix(0,(d+1),k)
  rownames(alpha) = c("intercept",colnames(X))
  colnames(alpha) = levels(Y)
  for (i in 1:k) {
    barXi=matrix(G[i,],d,1)
    alpha[1,i]=-t(barXi)%*%solve(W)%*%barXi
    alpha[2:(d+1),i]=2*solve(W)%*%barXi
  }
  return (alpha)
}
```

On ecrit la fonction predictY qui à partir de la matrice des alpha, predit la classe pour un tableau X.
```{r}
predictY<- function(X,alpha){
  s=as.matrix(cbind(1,X))%*%alpha
  Ypredit=colnames(alpha)[apply(s,1,which.max)]
}
```

On fait l'apprentissage du tableau de coefficient alpha :
```{r}
alpha = calcalpha(iristrain[,1:4], iristrain[,5])
alpha
```

On fait ensuite le classement sur l'échantillon test :
```{r}
Ypredit = predictY(iristest[,1:4], alpha)
```

```{r}
table(Y = iristest[,5], Ypredit)
```


On calcule le taux de bon classement (TBC) :
```{r}
TBC=mean(iristest[,5]==Ypredit)
TBC
```

Puis le taux de mauvais classement (TMC) :
```{r}
TMC = 1 - TBC
TMC
```

##3.
Par la validation leave-one-out, on procéde de manière similaire à ci-dessus; à chaqye étape, toutes les données sauf une servirons d'échantillon d'apprentissage, la donnée mise à l'écart servant d'échantillon test.
```{r}
Ypredit = rep("",nrow(iris))
for (i in 1:n){
  alpha = calcalpha(iris[-i,1:4],iris[-i,5])
  Ypredit[i] = predictY(iris[i,1:4],alpha)
}
table(Y = iris[,5], Ypredit)
```


On calcule le taux de bon classement (TBC) :
```{r}
TBC=mean(iris$Y==Ypredit)
TBC
```

Puis le taux de mauvais classement (TMC) :
```{r}
TMC = 1 - TBC
TMC
```

On obtient le même taux que celui de la question 1. Celui-ci ne doit pas être superieur à celui dans la question 1 (0,98) car ici on se trouve dans le cas non optimiste, avec séparation des données. 

#2. Analyse discriminante lineaire (iris de Fisher)
##1.
```{r}
#library("MASS")
```

A l'aide de la fonction lda, on va ajuster le modèle d'analyse discriminante linéaire.
```{r}
lda <- lda(X,grouping=Y,prior = prop.table(rep(1,nlevels(Y))))
lda
```

On calcule la matrice de confusion :
```{r}
Ypredit2 <- predict(lda)
table(Y,Ypredit = Ypredit2$class)
```

##2.
On évalue le taux de mauvais classement par validation croisée leave-one-out. On réalise aussi la matrice de confusion.
```{r}
LDA2 = lda(X,grouping=Y, CV=TRUE,prior = prop.table(rep(1,nlevels(Y))))
str(LDA2)
```

On affiche les probabilités a posteriori, sans faire appel à predict : 
```{r}
head(LDA2$posterior)
```

On affiche les classes d'affectation :
```{r}
table(Yreel=Y,LOO=LDA2$class)
```

On calcule le taux de bon classement (TBC) :
```{r}
TBC=mean(Y==LDA2$class)
TBC
```

Puis le taux de mauvais classement :
```{r}
TMC = 1 - TBC
TMC
```

##3. 
On utilise à nouveau la fonction lda sans préciser l'option CV=TRUE. 
```{r}
LDA3 = lda(X,grouping=Y,prior = prop.table(rep(1,nlevels(Y))))
LDA3
```
Dans les sorties, on remarque que la fonction retourne les coefficients linéaires discriminants LD1 et LD2.
Ceux-ci peuvent être récupérés par le champ scaling de l'objet retourné par la fonction lda :

```{r}
LDA3$scaling
```

En multipliant la matrice de données par la matrice des coefficients linéaires discriminants, on obtient une projection des individus sur ces axes discriminants: 

```{r}
d = as.matrix(iris[,1:4]) %*% LDA3$scaling
```

On fait le graphique permettant de visualiser ces données.
```{r}
plot(d, col = iris$Y)
```
On remarque qu'une des classes (noire) est isolée par rapport aux deux autres qui sont regroupées (verte et rouge).
