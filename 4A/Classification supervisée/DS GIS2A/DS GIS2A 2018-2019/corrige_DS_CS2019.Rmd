---
title: 'Devoir surveillé de classification supervisée : apGIS4'
author: "Vincent Vandewalle"
date: "04/04/2019"
output:
  pdf_document: default
  html_document: default
---

Durée 2h, tous documents autorisés


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Présentation des données 

On souhaite prédire la souscription à une assurance lors d'un vol aérien. Les variables sont les suivantes :
- Target: Claim Status (Claim)
- Name of agency (Agency)
- Type of travel insurance agencies (Agency.Type)
- Distribution channel of travel insurance agencies (Distribution.Channel)
- Name of the travel insurance products (Product.Name)
- Duration of travel (Duration)
- Destination of travel (Destination)
- Amount of sales of travel insurance policies (Net.Sales)
- Commission received for travel insurance agency (Commission)
- Gender of insured (Gender)
- Age of insured (Age)

La variable à prédire est la variable `Claim`

# Importation des données et premières analyses

*Q1 :* Importer les données à partir du fichier `assurance.csv`. On nommera `don` le data.frame résultant. Le jeu de données comporte-t'il des valeurs manquantes ? Quel option doit-on préciser dans R pour préciser la chaîne de caractères associée aux valeurs manquantes ? 

*R1 :*
```{r}
don = read.csv("assurance.csv",na.strings = "")
head(don)
summary(don)
# save(don, file = "don.Rda")
```

Par la suite on chargera le fichier `don.Rda` contenant le data.frame `don` pour être sûr de partir sur de bonnes bases.

*Q2 :* En quoi le problème qui vous est posé est-il un problème de classificiation supervisée ? Quel intérêt peut-il bien y avoir à prédire la variable `Claim` ?

*R2 :* Ici la variable à prédire est la variable `Claim` qui prend comme valeur soit `Yes` soit `No`, nous sommes donc bien dans le contexte de la prédiction d'une variable qualitative (binaire ici car uniquement deux modalités) à partir d'autres variables. Ici celà peut servir par exemple à identifier des clients plus succeptibles que d'autres de souscrire l'assurance puis de leur envoyer par exemple un courrier personnalisé.

*Q3 :* Dans vos données quelles sont les fréquences des différentes modalités de la variable `Claim` ? Dans quel ordre des modalités de la variable `Claim` sont-elles codées ?

*R3 :*
```{r}
prop.table(table(don$Claim))
levels(don$Claim)
```

Le première modalité du facteur `Claim` est donc `Yes` et la deuxième est `No`, donc quand on ajustera la régression logistique le phénomène qu'on prédira est $P(Claim = "Yes"|X = x)$ où $X$ représente l'ensemble des variables explicatives.

*Q4 :* Pouvez-vous donner une règle de classement qui a un taux de bon classement supérieur à 98%

*R4 :* Oui, il suffit de classer tous les individus dans la classe `No` (dans ce cas le taux de bon classement serait de 98,54%, la sensibilité de 0%, et la spécificité de 100%).

*Q5 :* En utilisant judicieusement les fonctions `sapply` et `nlevels` donner le nombre de modalités de chacune des variables. Que dire de la variable `Destination` ?

*R5 :*
```{r}
sapply(don, nlevels)
```
Ici la variable destination a 149 modalités, ce qui peut poser des problèmes ensuite dans les modèles prédictifs.

*Q6 :* Réaliser un test statistique permettant répondre à la question d'existance d'un lien entre la variable `Claim` et la variable `Agency`. Que conclure ? 

*R6 :*
```{r}
chisq.test(don$Claim, don$Agency)
```
On réalise un test du chi deux d'indépendance. Ici l'hypothèse nulle testée est l'indépendance des deux variables. Du fait de la sortie on rejetterai l'hypothèse nulle au risque $\alpha = 0,05$ (car p-value < 2.2e-16). Les variables sont donc *significativement* dépendantes. Ce qui augure une prédictibilité de la variable `Claim` par rapport à la variable `Agency` :)


*Q7 :* Dans la continuité de la question précédente afficher le vecteur contenant `P(Claim = Yes |Agency = x)` pour chacune des valeurs `x` de la variable agency, et la stocker dans une variable nommée `vecteur_proba`.

*R7 :*
```{r}
vecteur_proba = prop.table(table(don$Claim,don$Agency),2)[2,]
# save(vecteur_proba , file = "vecteur_proba.Rda")
```

*Q8 :* Les probabilités précédentes pourraient faire office de score, en associant à chaque individu la probabilité `P(Claim = Yes |Agency = x)` où `x` est la modalité dont dispose l'individu pour la variable `Agency`. En utilisant de manière adéquate l'indexation par nom, et en lançant une commande du type `vecteur_proba[don$Agency]`, obtenir le vecteur contenant les probabilités pour chaque individu. Enfin traçer la courbe ROC associée à ce score.

*R8 :* 
```{r}
library(ROCR)
Score = vecteur_proba[don$Agency] # Création du vecteur de score qui donne pour chaque individu P(Claim = Yes |Agency = x)
pred = prediction(Score,don$Claim)
perf = performance(pred,"tpr","fpr")
plot(perf)
```

*Q9 :* A votre avis si on avait effectué une régression logistique en lançant : 
`glm(Claim ~ Agence, family = "binomial", data = don)`, aurait-on obtenu les mêmes résultats ? Justifier.

*R9 :* Oui en fait dans ce cas particulier d'une seule variable explicative binaire la régression logistique est équivalentes aux probabilités issue du tableau des profils colonnes. 

Pour s'en convaincre on fait : 
```{r}
logit_agence = glm(Claim ~ Agency, family = "binomial", data = don)
vecteur_proba2 = predict(logit_agence, 
                 newdata = data.frame(Agency = levels(don$Agency)),
                 type = "response")
library(ROCR)
rbind(vecteur_proba,vecteur_proba2)
```
Ici les probabilité sont les mêmes à l'exception du cas ADM où on a 1.73e-07 contre 0, à relié peut-être à l'algorithme.


*Q10 :* On décide maintenant d'ajuster une régression logistique en prenant en compte toutes les variables. Après avoir lancé la commande adaptée on obtient les messages : 
a. `glm.fit: l'algorithme n'a pas convergé`, 
b. `glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1`
De plus en analysant les sorties 
c. `45107 observations deleted due to missingness`

Pour le message b. il s'agit du fait que dans la formule du calcul `P(Y = 1|X = x)` l'ordinateur indique que pour certains individus cette probabilité est ajusté numériquement à 0 ou 1. Ce n'est pas un problème en soi, mais cela peut parfois être un signe d'overfitting.

Pour le message a. pourquoi parle-t'on d'algorithme ici ?

Pour le message c. expliquer précisement pourquoi  45107 observation on été supprimée. Cela est-il bien grâve ? 

*R10 :*
message a. : ici l'algorithme utilisé est une algorithme iteratif (algorithme de Newton-Raphson), ici les message nous indique que le critère de convergence de l'algorithme n'est pas vérifié une fois le nombre maximal d'itération atteint. On pourrait l'éviter en augmentant le nombre l'obtenir en passant `maxit` de 25 (sa valeur par défaut) à une valeur plus élévée (par exemple 100).

message c. : Cela est dû 45107 valeurs manquantes pour la variable `Gender`, du coup cela conduit à se priver d'une bonne partie des données (45107/63326 = 71,2%). Il vaut mieux garder ces données, les valeurs manquantes pouvant être vues comme des valeurs particulières.

*Q11 :* On décide de récoder la variable `Gender` comme suit : 
```{r}
don$Gender = as.character(don$Gender)
don$Gender[is.na(don$Gender)] = "UNKNOWN"
don$Gender = factor(don$Gender)
table(don$Gender)
```

Que fait le code précédent, et quel intêret pour la suite peut-il bien avoir à effectuer ce recodage ? 

*R11 :* Le code précédant permet de recoder les valeurs manquantes comme une modalité particulière (modalité UNKNOWN) de la variable `Gender`. Cela permet par la suite des prendre en compte ces individus dans le modèle. Ce codage est tout à fait conforme puisque le fait de ne pas connaitre le Genre du passager peut être une information à par entière pour expliquer la souscription ou non de l'assurance.


*Q12 :* On relance maintenant l'ajustement du modèle complet (le code peut maintenant mettre un peu de temps à tourner au plus 2 min, sauvegarder avant de lancer ...). Tracer la courbe ROC associée et donner l'AUC.

*R12 :*
```{r}
mod_full = glm(Claim ~ ., family = "binomial", data = don)
pred = prediction(mod_full$fitted.values,don$Claim)
perf = performance(pred,"tpr","fpr")
plot(perf) # Tracé de la courbe ROC
performance(pred,"auc")@y.values[[1]] # Valeur de l'AUC
```


*Q13 :* Ici les classes sont en effectifs très déséquilibrés. Les stratégies usuelles peuvent alors être :
a. sous-échantillonner la classe en plus grand effectif
b. sur-échantillonner la classe en plus faible effectif
c. prendre directement en compte des poids dans l'ajustement du modèle, par exemple en prenant des poids inversement proportionnels à l'effectif des différentes classes

Les statégies a. et b. peuvent être mise en oeuvre en amont de l'ajustement du modèle, tandis que la stratégie c. nécessite l'utilisation de modèles capables de prendre en compte des poids.

D'un point de vue informatique la stratégie a. est la moins coûteuse en temps de calcul, mais elle conduit à perdre des données ... La stratégies b. conduirait à démultiplier le nombre de données ... Enfin la stratégie c. présenterait un coût informatique identique à l'ajustement du modèle classique (modula la prise en compte de poids) 

En fait, quel intérêt peut-il bien avoir dans toutes ces stratégies par rapport à pas tenir compte du tout de ce déséquilibre ?

*R13 :* Ici l'intérêt peut être de repondérer l'influence des données du groupe `Yes` par rapport au groupe `No`. Pour que les données de la classe `Yes` ne se retrouvent pas noyées par rapport à la classe `No` dans la régle de décision.


*Q14 :* Ici, compte-tenu de la faible capacité informatique dont nous disposont pour aujourd'hui on opte pour le sous-échantillonnage.

Fixer la graine du générateur de nombres pseudo-aléatoires à la valeur 1234.

Créer un nouveau jeu de données `don_us` comme "don under-sampled", constitué des 927 individus possédant la modalité `Yes` pour la variable `Claim` et de 927 individus tirés au hasard et sans remise parmi les 62399 individus possédant la modalité `No`. On pourra bien sûr, faire des selections pour distinguer les lignes avec `Yes` de celle avec `No`, utiliser la fonction `sample`, ainsi que la fonction`rbind` !


*R14 :*
```{r}
set.seed(1234)
don_yes = don[don$Claim == "Yes",] # Individus avec Yes 
don_no = don[don$Claim == "No",] # Ceux avec No
id_select_no = sample(nrow(don_no),nrow(don_yes)) # Choix au hasard des numéros d'individus parmi les `No`
# Autant que le nombre de `Yes`
don_us = rbind(don_yes, don_no[id_select_no,]) # Création du jeu de données don_us
save(don_us, file = "don_us.Rda")
```

Par la suite en cas de problème l'objet `don_us` est contenu dans le fichier `don_us.Rda` au besoin. 

*Q15 :* Ajuster maintenant le modèle de régression logistique à partir de toutes les variables sur le data.frame `don_us`. En regardant maintenant la sortie on obtient le message : 
`Coefficients: (9 not defined because of singularities)`, cela indique en particulier que dans la matrice du modèle, certaines colonnes peuvent être déduites comme combinaison linéaire d'autres colonnes. 
```{r}
reg_us = glm(Claim ~ ., family = "binomial", data = don_us)
```



Si on stocke les résultats l'ajustement précédent dans le variable `reg_us`, on peut récupérer la matrice du modèle comme suit :
```{r}
X = model.matrix(reg_us)
```

On s'intéresse aux valeurs propres de $X X^T$ (matrice à inverser en régression linéaire, et à peu de chose prés celle qui nécessite d'être inversée en régression logistique ...)
```{r}
eigen(t(X) %*% X)$values
```

Que nous indique des valeurs propres trés faibles quand à notre objectif d'inversion de cette matrice ? 

*R15 :* Ici on a certaines valeurs propres proches de 0, ce qui nous indique que la matrice est proche de la non-inversibilité, muticolinéarité des colonnes de $X$


*Q16 :*  A l'aide de la commande suivante on réalise une sélection pas à pas :
```{r}
min.model <- glm(Claim ~ 1, family = "binomial", data = don_us)
max.model <- glm(Claim ~ ., family = "binomial", data = don_us)
best.model = step(min.model, direction='both', 
                  scope= list(lower = min.model,
                              upper = max.model))
```

A quoi sert l'option direction = 'both' ? 
Quel intérêt dans la recherche peut-il y avoir à partir du modèle le plus simple ?

*R16 :* L'option direction = 'both' sert à faire la selection de variables forward-backward sur la base de l'optimisation du critère AIC. L'intérêt de commencer par le modèle le plus simple et d'éviter de passer par des modèles instables dans le processus d'optimisation de AIC (par exemple cas de variables avec trop de modalités).

*Q17 :* Interpréter le modèle retenu et évaluer ses performances (Courbe ROC, seuils effectuant le meilleur compromis sensibilité / spécificité, et sensibilité et spécificité associées)

*R17 :* Résumé du modèle : modèle retenu : 
`Claim ~ Product.Name + Net.Sales + Distribution.Channel + Age + Agency + Duration`

Résumé du modèle 
```{r}
summary(best.model)
```

Problème, 8 valeurs estimées à NA pour les modalités de la variables Agency : 
```{r}
coeff_pb = names(which(is.na(best.model$coefficients)))
coeff_pb
```

On regarde si ces modalités sont présentes dans le jeu de données `don_us`:
```{r}
table(don_us$Agency)
```

Elles sont bien présentes ! 

On doit donc avoir un problème de multicolénarité entre cette varaible et d'autres variables du modèle. Par exemple, si on croise les variable Agency et Product.Name on obtient le tableau : 
```{r}
tab = table(don_us$Agency, don_us$Product.Name)
tab
```

On peut pour mieux comprendre les choses on peut afficher la liste des agences pour lesquelles la connaissance du produit implique la connaissance de l'agence (profils colonnes égaux à 1).
```{r}
id = which((prop.table(tab,2) == 1), arr.ind = TRUE)
```

On obtient la liste ci-dessous : 
```{r}
cbind(rownames(tab)[id[,1]],colnames(tab)[id[,2]])
```
Par exemple sachant que le produit est 1 way Comprehansive Plan, on sait qu'il s'agit de l'agence EPX.

On peut aussi afficher la liste des agences concernées : 
```{r}
sort(unique(rownames(tab)[id[,1]]))
```

On retrouve bien la liste des agences concernée par le problème d'estimation des coefficients, modulo ART qui servait de modalité de référence pour la variable ART et qui n'apparait donc naturellement pas : 
```{r}
coeff_pb
```

Ainsi pour ces 8 agences, la connaissance du produit implique la connaissance de l'agence, l'information agence étant alors redondante par rapport à l'information produit. Dans ce cas R décide de ne pas estimer ces coefficients.  

On pourrait aussi croiser maintenant la variable avec `Distribution.Channel` : 
```{r}
tab2 = table(don_us$Agency, don_us$Distribution.Channel)
tab2
```
On remarque en fait qu'ici la connaissane de l'agence implique de façon quasi-déterministe la connaissance du réseau de distribution. Ainsi il est trés étonnant que cette variable ne soit pas supprimée ..., peut être du au problèmes d'ajustement du modèle déjà rencontrés avant. 

En pratique ce qui est fait lorsque deux variables sont trés dépendantes peut consister à créer une nouvelle variable à partir des deux variables précédentes : 
```{r}
library(tidyr) # pratique pour le nettoyage de données
don_us2 = don_us %>% unite(Agency.Product,Agency,Product.Name)
table(don_us2$Agency.Product) 
```

Le regression logistique ne posant maintenant plus de problème :
```{r}
min.model <- glm(Claim ~ 1, family = "binomial", data = don_us2)
max.model <- glm(Claim ~ ., family = "binomial", data = don_us2)
best.model2 <- step(min.model, direction='both', 
                  scope= list(lower = min.model,
                              upper = max.model))
coefficients(best.model2)
formula(best.model2)
```
avec des coefficients ayants tous les coefficient avec des valeurs estimées différentes de NA.

On peut aussi regarder le résummé du modèle : 
```{r}
summary(best.model2)
```

Ici les coefficients dans l'ensemble restent plutôt trés mals estimés, grande variance des estimateurs, du surement au grand nombre de modalités. Et donc peut de coefficients apparaissent comme significatifs. 

Dans ce cas on pourrait essayer de réduire le nombre de modalités de la variable Agency.Product créée, par exemple sur la base de test du chi-2 (similaire aux arbre CHAID)

```{r}
library(CHAID)
# On crée un arbre CHAID avec uniquement la variable Agency.Product 
# recodée en qualitative
chaid_tree <- chaid(Claim ~ as.factor(Agency.Product), don_us2, 
      control = chaid_control(alpha2 = 0.05))
# On recupères les feuilles associées à chacune des données
feuilles_chaid <- names(predict(chaid_tree))
# Distribution des effectifs associés à chacun des classements
table(feuilles_chaid)
# Croisement avec la variable Agency.Product
table(feuilles_chaid, don_us2$Agency.Product)
# On crée un nouveau data.frame en remplaçant Agency.Product par les modalités regroupées
don_us3 <- don_us2
don_us3 <- don_us3 %>% dplyr::select(-Agency.Product) %>% 
  dplyr::mutate(Agency.Product.Grouped = factor(feuilles_chaid))
```

Enfin on peut réajuster le modèle de régression logistique : 
```{r}
min.model3 <- glm(Claim ~ 1, family = "binomial", data = don_us3)
max.model3 <- glm(Claim ~ ., family = "binomial", data = don_us3)
best.model3 <- step(min.model3, direction='both', 
                  scope= list(lower = min.model3,
                              upper = max.model3))
summary(best.model3)
```

On récupère maintenant des coefficients estimés bien définis (plus de variance démesurée). 

Pas si simple que ça de développer un score ... 

```{r}
pred_best3 = prediction(best.model3$fitted.values,don_us3$Claim)
perf_best3 = performance(pred_best3,"tpr","fpr")
plot(perf_best3) # Tracé de la courbe ROC
performance(pred_best3,"auc")@y.values[[1]] # Valeur de l'AUC
```

Courbe qu'on aurait aussi pu comparer à celle du premier modèle : 
```{r}
pred_best = prediction(best.model$fitted.values,don_us$Claim)
perf_best = performance(pred_best,"tpr","fpr")
plot(perf_best) # Tracé de la courbe ROC
performance(pred_best,"auc")@y.values[[1]] # Valeur de l'AUC
```

Les résultats sont somme toutes trés similaire que dans le cas du modèle 3, mais avec l'interprétabilité en plus.


*Q18 :* On souhaite ajuster un modèle de forêts aléatoires à l'aide de la commande : 
```{r, eval = FALSE}
library(randomForest)
randomForest(Claim ~ ., data = don_us)
```
On obtient le message d'erreur 
`Error in randomForest.default(m, y, ...) : Can not handle categorical predictors with more than 53 categories.`

Adapter le code pour pouvoir ajuster les random forests, et evaluer les performances de celles-ci.

*R18 :* Il suffit simplement de supprimer la variable département (variable numéro 7)
```{r}
library(randomForest)
rf = randomForest(Claim ~ ., data = don_us[-7])
pred_rf = prediction(rf$votes[,2],don_us$Claim)
perf_rf = performance(pred_rf,"tpr","fpr")
plot(perf_rf) # Tracé de la courbe ROC
performance(pred_rf,"auc")@y.values[[1]] # Valeur de l'AUC
```

Résultats très similaire ici.

On peut s'intéresser à l'importance des différentes variables
```{r}
rf$importance[order(rf$importance, decreasing = T),,drop = F]
```

*Q19 :*  On aurait pu vouloir ajuster un modèle d'analyse discriminante probabiliste. Pourquoi ne peut-on pas utiliser ici la LDA ou la QDA ?

*R19 :* Ici LDA et QDA ne sont pas envisageables si toutes les variables ne sont pas quantitatives, donc si on veut faire usage de toutes les variables à notre disposition (qualitatives y compris) on ne peut pas faire usage de LDA et QDA.


*Q20 :* Ici on souhaite ajuster un classifieur de Bayes naïf à l'aide de la fonction `naiveBayes` du package `e1071`. Réaliser cet ajustement sur le jeu de données `don`, que remarquez vous sur le temps d'excusion par rapport au temps précédent ? Comment expliquez-vous celà ?

*R20 :*
```{r}
library(e1071)
nb_don = naiveBayes(Claim ~ .,data = don)
```
Ici l'exécution de l'algorithme est instantanée, simples calculs de fréquences, variances, et moyennes.

*Q21 :** Le classifieur de Bayes naïf (cas particulier de méthode d'analyse discriminante probabiliste) est basé sur l'estimation des $P(Y = i) = \pi_i$ et $P(X = x|Y =i) = f_i(x)$, puis de l'application du théorème de Bayes. 

En passant de `don` à `don_us`, comment l'estimation de $\pi_0$ et $\pi_1$ est-elle modifiée ? Les $f_0(x)$ et $f_1(x)$ estimés sont-ils foncièrement différents entre `don` et `don_us` ? Justifier.

*R21 :* Pour `don_us` les proportions estimée sont $\hat\pi_1 = \hat\pi_0 = 0,5$ tandis que pour `don` on a   $\hat\pi_1 \simeq 0,01$, $\hat\pi_0 \simeq 0,99$. En fait ici les $f_1(x)$ estimés sont strictement les mêmes (strictement mêmes données pour l'estimation), pour $f_0(x)$ comme le tirage est fait selon un échantillonnage aléatoire simple, l'estimateur produit à partir du sous-echantillon à les mêmes propriétés asymptotique que l'estimateur à par de l'échantillon complet, par conséquent on devrait avoir des valeurs relativement simulaires pour `don` et don_us`.

```{r}
nb_don_us = naiveBayes(Claim ~ .,data = don_us)
# Comparaison pour la variable Agency.Type
nb_don$tables$Agency.Type
nb_don_us$tables$Agency.Type
# Comparaison pour la variable Age
nb_don$tables$Age
nb_don_us$tables$Age
```


*Q22 :* En partant de la formule de Bayes, montrer que la probabilité $P(Y = 1 |X = x)$ est une fonction croissante du rapport $f_1(x)/f_0(x)$. 

*R22 :* 
$$
P(Y = 1 |X = x) = \frac{\pi_1 f_1(x)}{\pi_0 f_0(x) + \pi_1 f_1(x)} = \frac{\pi_1 \frac{f_1(x)}{f_0(x)}}{\pi_0 + \pi_1 \frac{f_1(x)}{f_0(x)}} = 1 - \frac{1}{\pi_0 + \pi_1 \frac{f_1(x)}{f_0(x)}}
$$
Ici comme $\frac{f_1(x)}{f_0(x)}>0$, $\pi_1 > 0$, on en déduit facilement que $P(Y = 1 |X = x)$ est une fonction croissante du rapport $\frac{f_1(x)}{f_0(x)}$ 


*Q23 :* En déduire que les valeurs estimées de $\pi_0$ et $\pi_1$ n'influent en rien sur l'ordre dans lequel seront rangés les individus en terme de probabilités.

*R23 :* Ici, dans l'expression de $P(Y = 1 |X = x)$, la seule quantité qui dépend de $x$ est le rapport $\frac{f_1(x)}{f_0(x)}$, et on sait d'après la question précédente que $P(Y = 1 |X = x)$ est une fonction croissante de ce rapport. Donc $\pi_0$ et $\pi_1$ n'influent en rien sur l'ordre dans lequel seront rangés les individus en termes de probabilités.

*Q24 :* Conclure des question Q21 à Q23 que la courbe ROC est théoriquement inchangée entre les version répondérées ou non pour le cas de l'analyse discriminate probabiliste, dans le cas où les échantillons sont de grandes tailles (considérations asymptotiques oblige ...)

*R24 :* Les individus devraient être rangés à peu près de la même manière avec ou sans sous-échantillonnage, d'après les questions précédentes (puisque moralement seuls les estimations de $\pi_1$ et $\pi_0$ différent entre l'échantillon `don` et `don_us`). Donc la ROC qui ne dépend que de la façons dont les individus sont ordonnés selon leur score est théoriquement inchangée en passant de de `don` à `don_us`.

*Q25 :* En conséquence expliquer pourquoi on a tout intérêt à plutôt utiliser `don` que `don_us`

*R25 :* On a tout intérêt à utiliser `don` tout entier puisque $f_0(x)$ sera mieux estimé avec plus de données.


*Q26 :* Parmis les évaluations réalisées pour les différents modèles, les classer de la plus sujette à une sur-évaluation des performances (biais d'optimisme), à celle la mois sujette à cette sur-évaluation. Préciser éventuellement si certaines ne sont pas sujettes du tout à ce biais.

*R26 :* Ici la méthode la plus sujette à la sur-évaluation des performance est la régression logistique, ensuite le classifieur de Bayes Naïf, et enfin les forêt aléatoires ne sont pas sujette à la sur-évaluation des performances puisque l'on utilise l'out-of-bag pour estimer les performances.

*Q27 :* Proposez une approche permettant de comparer "en toute honnêteté" les différents modèles proposés (implémentation non demandée)

*R27 :* Ici la bonne méthodes consiste à séparer l'échantillon entre un enchantillon d'apprentissage et un échantillon test, apprendre sur l'apprentissage et tester sur le test. 

```{r}
id_app = sample(nrow(don),0.7*nrow(don))
don_app = don[id_app,]
don_test = don[-id_app,]
```

Attention : ici on repart des données 

On compare les modèles :
- M1 : Régression logistique classique + stepwise
- M2 : Régression logistique avec sous échantillonnage + stepwise
- M3 : Régression logistique avec sous échantillonnage +  croisement + regroupement de modalités + stepwise
- M4 : Forêts aléatoires échantillon complet
- M5 : Forêts aléatoires avec sous échantillonnage
- M6 : Naive Bayes échantillon complet
- M7 : Naive Bayes sous-échanillonnage

M1 : 
```{r}
min.model_M1 <- glm(Claim ~ 1, family = "binomial", data = don_app)
max.model_M1 <- glm(Claim ~ ., family = "binomial", data = don_app)
M1 = step(min.model_M1, direction='both', 
                  scope= list(lower = min.model_M1,
                              upper = max.model_M1))
p_M1 <- predict(M1,don_app, type = "response")
```

M2 : 
```{r}
# Sous-échantillonnage
don_app_yes = don[don_app$Claim == "Yes",] 
don_app_no = don[don_app$Claim == "No",]
id_select_app_no = sample(nrow(don_app_no),nrow(don_app_yes))
don_app_us = rbind(don_app_yes, don_app_no[id_select_app_no,])
# Fitting :
min.model_M2 <- glm(Claim ~ 1, family = "binomial", data = don_app_us)
max.model_M2 <- glm(Claim ~ ., family = "binomial", data = don_app_us)
M2 = step(min.model_M2, direction='both', 
                  scope= list(lower = min.model_M2,
                              upper = max.model_M2))
p_M2 <- predict(M2,don_app, type = "response")
```

M3 : 
```{r}
don_app_us2 = don_app_us %>% unite(Agency.Product,Agency,Product.Name)
app_chaid_tree <- chaid(Claim ~ as.factor(Agency.Product), 
                        don_app_us2 , 
                        control = chaid_control(alpha2 = 0.000000001)) # seuil à fixer à la main
app_feuilles_chaid <- names(predict(app_chaid_tree))
#table(app_feuilles_chaid)
# table(app_feuilles_chaid, don_app_us2$Agency.Product)
don_app_us3 <- don_app_us2
don_app_us3 <- don_app_us3 %>% dplyr::select(-Agency.Product) %>% 
  dplyr::mutate(Agency.Product.Grouped = factor(app_feuilles_chaid))
min.model_M3 <- glm(Claim ~ 1, family = "binomial", data = don_app_us3)
# Suppression de niveau avec une seule modalité
don_app_us3 = don_app_us3[sapply(don_app_us3,nlevels) != 1]
max.model_M3 <- glm(Claim ~ ., family = "binomial", data = don_app_us3)
M3 = step(min.model_M3, direction='both', 
                  scope= list(lower = min.model_M3,
                              upper = max.model_M3))
summary(M3)
p_M3 <- predict(M3,don_app, type = "response")
```














