plot(hcpc,choice="map",draw.tree = F)
setwd("~/4A/AD/TP_ACM")
#----------------------------------------------------------------------
#-------------------- TP 2 : AMC -----------------------
#----------------------------------------------------------------------
#Importation des packages
library("FactoMineR")
library("psych")
library("Hmisc")
library(factoextra)
library(readxl)
#----------Lecture et préparation des données----------
mydata=read.table("Race-canine.txt",sep="\t", head=T,encoding = "latin1", colClasses = "factor" )
head(mydata)
rownames(mydata) <- mydata$Race
mydata<-mydata[,-1]
head(mydata)
#-----------Realisation de l'ACM-----------
library(ggplot2)
mydata.mca = MCA(mydata, graph=FALSE,quali.sup = 7,ncp=3)
library(plyr)
#-----------Analyse des résultats-----------
#Valeurs propres
#sous forme de tableau
mydata.mca$eig
#sous forme de graphique
fviz_eig(mydata.mca, addlabels = TRUE)
#Inertie moyenne (Critère de Kaiser)
sum(mydata.mca$eig[,2]>(100/nrow(mydata.mca$eig)), na.rm=TRUE)
#ou
#sum(mydata.mca$eig[,1]>(sum(mydata.mca$eig[,1])/nrow(mydata.mca$eig)), na.rm=TRUE)
#Graphique
fviz_screeplot (mydata.mca) +
geom_hline (yintercept = 100/nrow(mydata.mca$eig), linetype = 2, color = "red")
#Inertie totale
which(mydata.mca$eig[,3]>80)[1]
#Graphique
barplot(mydata.mca$eig[,3])
lines(c(0,20),c(80,80))
##Q2##
mydata.mca$var$contrib
mydata.mca$var$cos2
mydata.mca$var$coord
sign(mydata.mca$var$coord)
#Q 3
Taille=colSums(mydata.mca$var$contrib[1:3,])
Poid=colSums(mydata.mca$var$contrib[4:6,])
Velocité=colSums(mydata.mca$var$contrib[7:9,])
Intellegence=colSums(mydata.mca$var$contrib[10:12,])
Affection=colSums(mydata.mca$var$contrib[12:14,])
Agressivité=colSums(mydata.mca$var$contrib[14:16,])
rbind(Taille,Poid,Velocité,Intellegence,Affection,Agressivité)
#Q4
nrow(mydata)
mydata.mca$ind$contrib
mydata.mca$ind$cos2
mydata.mca$ind$coord
sign(mydata.mca$ind$coord)
#Q5
mydata.mca$quali.sup
#Q6
#Graphique individus
fviz_mca_ind(mydata.mca,)
#Graphique variables catégories
fviz_mca_var(mydata.mca, repel = TRUE)
#Map Dim1 et 2
plot(mydata.mca, axes = c(1,2), choix="ind", invisible = c("var","quali.sup"))
?plot.MCA
#CAH
library(ggplot2)
library(plyr)
library(philentropy)
library(factoextra)
data.hcpc <- HCPC(mydata.mca, nb.clust = 4, proba = 1, graph = FALSE)
plot(data.hcpc, choice="bar")
plot(data.hcpc, choice="tree")
data.hcpc$desc.var$test.chi2
data.hcpc$desc.var$category$'4'
barplot(data.hcpc$desc.var$category$'1'[,1])
plot(hcpc,choice="map",draw.tree = F)
plot(hcpc,choice="map",draw.tree = F)
plot(data.hcpc,choice="map",draw.tree = F)
plot(data.hcpc,choice="map",draw.tree = F)
setwd("~/4A/AD/TP_ACM")
#----------------------------------------------------------------------
#-------------------- TP 1 : ANALYSE DE DONNEES -----------------------
#----------------------------------------------------------------------
#-------------------------------Exercice 2-----------------------------
#-----------------------importer un fichier----------------------------
#------------ librairie necessaire pour faire une acp -----------------
library("FactoMineR")
library("psych")
library("Hmisc")
library(factoextra)
#------------ librairie necessaire pour lire un fichier excel----------
#install.packages("readxl")
library(readxl)
#----------------------importation des données-------------------------
mydata <- read_excel("Data_eleves.xls")
#Suppression des données vides
mydata<-mydata[-(28:30),-16]
#---------------------- Etude statistiques ----------------------------
# Pour avoir un aperçu des données statistiques utilisées ainsi que leur type
str(mydata)
# Pour avoir un resumé des données
summary(mydata)
#preparation des données
mydata<-data.frame(mydata)
rownames(mydata) <- mydata$eleves
mydata<-mydata[,-1]
head(mydata)
# Pour obtenir les variables, la quantité, la moyenne, la médiane...
psych::describe(mydata)
#-------------------- Matrice de correlation --------------------------
# Permet d'obetnir la matrice de corrélation ainsi que les p-valeurs
rcorr(as.matrix(mydata))
# La matrice de corrélation est symétrique. La diagonale est nulle.
# Plus la corrélation est proche de 1, plus les matières sont corrélées.
# Par exemple, Arts et Education musicale semble fortement corrélées (corrélation = 0.70)
# 5% d'erreur = on prend les valeurs inférieures à 0.05 dans la matrice des p-valeurs pour valider.
# La p-value des notes d'arts et d'éducation musicale est 0, donc les notes d'arts et d'éducation musicale sont fortement corrélées.
# Cela signifie que les notes évoluent dans le même sens (même note en arts semble indiquer bonne note en éducation musicale).
# Les coefficients les plus importants sont :
# Orthographe et anglais (corrélation : 0.65)
# Expression et histoire (corrélation : 0.59)
# Expression et éducation musicale (corrélation : 0.63)
# Expression et arts (corrélation : 0.72)
# Maths et expression (corrlation : 0.59)
# Maths et anglais (corrélation : 0.70)
# Maths et histoire (corrélation : 0.64)
# Anglais et histoire (corrélation : 0.59)
# Arts et éducation musicale (corrélation : 0.70)
# En regardant les p-valeurs, on peut dire que les matières suivantes sont corrélées (5% d'erreur) :
# Orthographe et anglais (p-value : 0.0002)
# Expression et histoire (p-value : 0.0012)
# Expression et éducation musicale (p-value : 0.0004)
# Expression et arts (p-value : 0.0000)
# Maths et expression (p-value : 0.0012)
# Maths et anglais (p-value : 0.0000)
# Maths et histoire (p-value : 0.0003)
# Anglais et histoire (p-value : 0.0011)
# Arts et éducation musicale (p-value : 0.0000)
#------------------------------- ACP ----------------------------------
# On fait l'ACP
mydata.pca = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE)
# On regarde comment est composÃ© mydata.pca
str(mydata.pca)
#Pour accéder aux valeurs propres et à leur pourcentage
mydata.pca$eig
# On a généré 6 composantes principales
# On a dÃ©composÃ© la trace de la matrice en 6 .. On passe donc d'un tableau Ã  6 variables Ã  un graphique avec deux axes reprÃ©senant toutes les variables....
# On retient le minimum de composantes principales...
# La premiÃ¨re composante contient 44,98% de l'information...
# La premiÃ¨re + la deuxiÃ¨me composantes contiennes 70,7% de l'information...
# La premiÃ¨re + la deuxiÃ¨me + troisiÃ¨me composantes contiennes 83% de l'information...
# On cherche plan de projection donc avec le maximum d'information possible pour le moindre coÃ»t...
# On veut que l'image projettÃ©e soit fidÃ¨le au modÃ¨le..
# =>Ici on est pret Ã  prendre 83% de l'information pour reprÃ©senter les composantes.
# On va y ajouter de nouvelles choses :
# On sait que la trace de la matrice = 6 (somme diago =6). Donc 1/6 * la somme des diago =1 (on a calculÃ© une moyenne).
# Donc on va mettre un seuil pour dÃ©finir d'accepter ou non une composante.
# On va donc conserver la valeur propre si elle est supÃ©rieur Ã  1.
# => Donc ici on va seulement conserver 2 valeurs propres (les deux premiÃ¨res). => C'est la mÃ©thode de KAISER.
# RQ : InterpretatbilitÃ© ... Quand on a 8/9.. On peut discuter d'accepter ou on la composante principale...
# Dans notre cas on va devoir accepter la troisiÃ¨me composante principale pour recupÃ©rer et obtenir 80% de l'information.
#Pour accÃ¨der aux variances
mydata.pca$var
contribution_moy_theo <- 100/nrow(mydata);
#Pour accÃ¨der aux ..
mydata.pca$ind
#Contributions
mydata.pca$ind$coord
# Pour avoir le tableau du poly page 52
mydata.pca$var$cos2
# On affiche le graphique de l'acp
plot.PCA(mydata.pca, axes=c(1,2),choix="var")
plot.PCA(mydata.pca, axes=c(3,1),choix="var")
plot.PCA(mydata.pca, axes=c(2,3),choix="var")
plot.PCA(mydata.pca, axes=c(1,2),choix="ind")
# Pour aller plus loins :
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, quanti.sup = 1)
plot.PCA(mydata.pcaN, axes=c(1,2),choix="var")
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, ind.sup = c(1,2))
plot.PCA(mydata.pcaN, axes=c(1,2),choix="ind")
# POUR AVOIR LE GRAPHIQUE DES VALEURS PROPRES :
fviz_eig(mydata.pca, addlabels = TRUE, ylim = c(0, 50))
#Coupure a dim=3 donc on garde 3 facteurs
# Avec le critÃ¨re que Kaiser, on garde 3 axes qui reprÃ©sente Ã  eux 83% de la variance
fviz_eig(mydata.pca,choice = "eigenvalue", addlabels = TRUE )+ geom_hline (yintercept = 1, linetype = 2, color= "red")
barplot(mydata.pca$eig[,3]) + lines(c(0,20),c(80,80),type = "l", lty=2,col="red")
#Realisation CAH
#install.packages("ggplot")
library(ggplot)
library(plyr)
library(philentropy)
library(factoextra)
#diviser en 4 clusters
data.hcpc <- HCPC(mydata.pca, nb.clust = 4, proba = 1, graph=FALSE)
#visualiser gain d'inertie
plot(data.hcpc, choice="bar")
#diminuer nombre de clisters à 3
data.hcpc <- HCPC(mydata.pca, nb.clust = 3, proba = 1, graph=FALSE)
#pour avoir les groupes/classes
plot(data.hcpc, choice="tree")
#analyser les vars les + explicites pour les classes
data.hcpc$desc.var$quanti$'1'
data.hcpc$desc.var$quanti$'2'
data.hcpc$desc.var$quanti$'3'
pl1 <- fviz_cluster(data.hcpc, ellipse = FALSE )
fviz_add(pl1, df=mydata, axes = c(1,2))
?fviz_add
?fviz_cluster
plot(data.hcpc,choice="map",draw.tree = F)
plot(data.hcpc,choice="map",draw.tree = F)
#----------------------------------------------------------------------
#-------------------- TP 1 : ANALYSE DE DONNEES -----------------------
#----------------------------------------------------------------------
#-------------------------------Exercice 2-----------------------------
#-----------------------importer un fichier----------------------------
#------------ librairie necessaire pour faire une acp -----------------
library("FactoMineR")
library("psych")
library("Hmisc")
library(factoextra)
#------------ librairie necessaire pour lire un fichier excel----------
#install.packages("readxl")
library(readxl)
#----------------------importation des données-------------------------
mydata <- read_excel("Data_eleves.xls")
#Suppression des données vides
mydata<-mydata[-(28:30),-16]
#---------------------- Etude statistiques ----------------------------
# Pour avoir un aperçu des données statistiques utilisées ainsi que leur type
str(mydata)
# Pour avoir un resumé des données
summary(mydata)
#preparation des données
mydata<-data.frame(mydata)
rownames(mydata) <- mydata$eleves
mydata<-mydata[,-1]
head(mydata)
# Pour obtenir les variables, la quantité, la moyenne, la médiane...
psych::describe(mydata)
#-------------------- Matrice de correlation --------------------------
# Permet d'obetnir la matrice de corrélation ainsi que les p-valeurs
rcorr(as.matrix(mydata))
# La matrice de corrélation est symétrique. La diagonale est nulle.
# Plus la corrélation est proche de 1, plus les matières sont corrélées.
# Par exemple, Arts et Education musicale semble fortement corrélées (corrélation = 0.70)
# 5% d'erreur = on prend les valeurs inférieures à 0.05 dans la matrice des p-valeurs pour valider.
# La p-value des notes d'arts et d'éducation musicale est 0, donc les notes d'arts et d'éducation musicale sont fortement corrélées.
# Cela signifie que les notes évoluent dans le même sens (même note en arts semble indiquer bonne note en éducation musicale).
# Les coefficients les plus importants sont :
# Orthographe et anglais (corrélation : 0.65)
# Expression et histoire (corrélation : 0.59)
# Expression et éducation musicale (corrélation : 0.63)
# Expression et arts (corrélation : 0.72)
# Maths et expression (corrlation : 0.59)
# Maths et anglais (corrélation : 0.70)
# Maths et histoire (corrélation : 0.64)
# Anglais et histoire (corrélation : 0.59)
# Arts et éducation musicale (corrélation : 0.70)
# En regardant les p-valeurs, on peut dire que les matières suivantes sont corrélées (5% d'erreur) :
# Orthographe et anglais (p-value : 0.0002)
# Expression et histoire (p-value : 0.0012)
# Expression et éducation musicale (p-value : 0.0004)
# Expression et arts (p-value : 0.0000)
# Maths et expression (p-value : 0.0012)
# Maths et anglais (p-value : 0.0000)
# Maths et histoire (p-value : 0.0003)
# Anglais et histoire (p-value : 0.0011)
# Arts et éducation musicale (p-value : 0.0000)
#------------------------------- ACP ----------------------------------
# On fait l'ACP
mydata.pca = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE)
# On regarde comment est composÃ© mydata.pca
str(mydata.pca)
#Pour accéder aux valeurs propres et à leur pourcentage
mydata.pca$eig
# On a généré 6 composantes principales
# On a dÃ©composÃ© la trace de la matrice en 6 .. On passe donc d'un tableau Ã  6 variables Ã  un graphique avec deux axes reprÃ©senant toutes les variables....
# On retient le minimum de composantes principales...
# La premiÃ¨re composante contient 44,98% de l'information...
# La premiÃ¨re + la deuxiÃ¨me composantes contiennes 70,7% de l'information...
# La premiÃ¨re + la deuxiÃ¨me + troisiÃ¨me composantes contiennes 83% de l'information...
# On cherche plan de projection donc avec le maximum d'information possible pour le moindre coÃ»t...
# On veut que l'image projettÃ©e soit fidÃ¨le au modÃ¨le..
# =>Ici on est pret Ã  prendre 83% de l'information pour reprÃ©senter les composantes.
# On va y ajouter de nouvelles choses :
# On sait que la trace de la matrice = 6 (somme diago =6). Donc 1/6 * la somme des diago =1 (on a calculÃ© une moyenne).
# Donc on va mettre un seuil pour dÃ©finir d'accepter ou non une composante.
# On va donc conserver la valeur propre si elle est supÃ©rieur Ã  1.
# => Donc ici on va seulement conserver 2 valeurs propres (les deux premiÃ¨res). => C'est la mÃ©thode de KAISER.
# RQ : InterpretatbilitÃ© ... Quand on a 8/9.. On peut discuter d'accepter ou on la composante principale...
# Dans notre cas on va devoir accepter la troisiÃ¨me composante principale pour recupÃ©rer et obtenir 80% de l'information.
#Pour accÃ¨der aux variances
mydata.pca$var
contribution_moy_theo <- 100/nrow(mydata);
#Pour accÃ¨der aux ..
mydata.pca$ind
#Contributions
mydata.pca$ind$coord
# Pour avoir le tableau du poly page 52
mydata.pca$var$cos2
# On affiche le graphique de l'acp
plot.PCA(mydata.pca, axes=c(1,2),choix="var")
plot.PCA(mydata.pca, axes=c(3,1),choix="var")
plot.PCA(mydata.pca, axes=c(2,3),choix="var")
plot.PCA(mydata.pca, axes=c(1,2),choix="ind")
# Pour aller plus loins :
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, quanti.sup = 1)
plot.PCA(mydata.pcaN, axes=c(1,2),choix="var")
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, ind.sup = c(1,2))
plot.PCA(mydata.pcaN, axes=c(1,2),choix="ind")
# POUR AVOIR LE GRAPHIQUE DES VALEURS PROPRES :
fviz_eig(mydata.pca, addlabels = TRUE, ylim = c(0, 50))
#Coupure a dim=3 donc on garde 3 facteurs
# Avec le critÃ¨re que Kaiser, on garde 3 axes qui reprÃ©sente Ã  eux 83% de la variance
fviz_eig(mydata.pca,choice = "eigenvalue", addlabels = TRUE )+ geom_hline (yintercept = 1, linetype = 2, color= "red")
barplot(mydata.pca$eig[,3]) + lines(c(0,20),c(80,80),type = "l", lty=2,col="red")
#Realisation CAH
#install.packages("ggplot")
library(ggplot)
library(plyr)
library(philentropy)
library(factoextra)
#diviser en 4 clusters
data.hcpc <- HCPC(mydata.pca, nb.clust = 4, proba = 1, graph=FALSE)
#visualiser gain d'inertie
plot(data.hcpc, choice="bar")
#diminuer nombre de clisters à 3
data.hcpc <- HCPC(mydata.pca, nb.clust = 3, proba = 1, graph=FALSE)
#pour avoir les groupes/classes
plot(data.hcpc, choice="tree")
#analyser les vars les + explicites pour les classes
data.hcpc$desc.var$quanti$'1'
data.hcpc$desc.var$quanti$'2'
data.hcpc$desc.var$quanti$'3'
pl1 <- fviz_cluster(data.hcpc, ellipse = FALSE )
fviz_add(pl1, df=mydata, axes = c(1,2))
?fviz_add
?fviz_cluster
plot(data.hcpc,choice="map",draw.tree = F)
plot(data.hcpc,choice="map",draw.tree = F)
#----------------------importation des données-------------------------
mydata <- read_excel("Data_eleves.xls")
#------------ librairie necessaire pour faire une acp -----------------
library("FactoMineR")
library("psych")
library("Hmisc")
library(factoextra)
#------------ librairie necessaire pour lire un fichier excel----------
#install.packages("readxl")
library(readxl)
#----------------------importation des données-------------------------
mydata <- read_excel("Data_eleves.xls")
setwd("~/4A/AD/TP_ACP")
#----------------------------------------------------------------------
#-------------------- TP 1 : ANALYSE DE DONNEES -----------------------
#----------------------------------------------------------------------
#-------------------------------Exercice 2-----------------------------
#-----------------------importer un fichier----------------------------
#------------ librairie necessaire pour faire une acp -----------------
library("FactoMineR")
library("psych")
library("Hmisc")
library(factoextra)
#------------ librairie necessaire pour lire un fichier excel----------
#install.packages("readxl")
library(readxl)
#----------------------importation des données-------------------------
mydata <- read_excel("Data_eleves.xls")
#Suppression des données vides
mydata<-mydata[-(28:30),-16]
#---------------------- Etude statistiques ----------------------------
# Pour avoir un aperçu des données statistiques utilisées ainsi que leur type
str(mydata)
# Pour avoir un resumé des données
summary(mydata)
#preparation des données
mydata<-data.frame(mydata)
rownames(mydata) <- mydata$eleves
mydata<-mydata[,-1]
head(mydata)
# Pour obtenir les variables, la quantité, la moyenne, la médiane...
psych::describe(mydata)
#-------------------- Matrice de correlation --------------------------
# Permet d'obetnir la matrice de corrélation ainsi que les p-valeurs
rcorr(as.matrix(mydata))
# La matrice de corrélation est symétrique. La diagonale est nulle.
# Plus la corrélation est proche de 1, plus les matières sont corrélées.
# Par exemple, Arts et Education musicale semble fortement corrélées (corrélation = 0.70)
# 5% d'erreur = on prend les valeurs inférieures à 0.05 dans la matrice des p-valeurs pour valider.
# La p-value des notes d'arts et d'éducation musicale est 0, donc les notes d'arts et d'éducation musicale sont fortement corrélées.
# Cela signifie que les notes évoluent dans le même sens (même note en arts semble indiquer bonne note en éducation musicale).
# Les coefficients les plus importants sont :
# Orthographe et anglais (corrélation : 0.65)
# Expression et histoire (corrélation : 0.59)
# Expression et éducation musicale (corrélation : 0.63)
# Expression et arts (corrélation : 0.72)
# Maths et expression (corrlation : 0.59)
# Maths et anglais (corrélation : 0.70)
# Maths et histoire (corrélation : 0.64)
# Anglais et histoire (corrélation : 0.59)
# Arts et éducation musicale (corrélation : 0.70)
# En regardant les p-valeurs, on peut dire que les matières suivantes sont corrélées (5% d'erreur) :
# Orthographe et anglais (p-value : 0.0002)
# Expression et histoire (p-value : 0.0012)
# Expression et éducation musicale (p-value : 0.0004)
# Expression et arts (p-value : 0.0000)
# Maths et expression (p-value : 0.0012)
# Maths et anglais (p-value : 0.0000)
# Maths et histoire (p-value : 0.0003)
# Anglais et histoire (p-value : 0.0011)
# Arts et éducation musicale (p-value : 0.0000)
#------------------------------- ACP ----------------------------------
# On fait l'ACP
mydata.pca = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE)
# On regarde comment est composÃ© mydata.pca
str(mydata.pca)
#Pour accéder aux valeurs propres et à leur pourcentage
mydata.pca$eig
# On a généré 6 composantes principales
# On a dÃ©composÃ© la trace de la matrice en 6 .. On passe donc d'un tableau Ã  6 variables Ã  un graphique avec deux axes reprÃ©senant toutes les variables....
# On retient le minimum de composantes principales...
# La premiÃ¨re composante contient 44,98% de l'information...
# La premiÃ¨re + la deuxiÃ¨me composantes contiennes 70,7% de l'information...
# La premiÃ¨re + la deuxiÃ¨me + troisiÃ¨me composantes contiennes 83% de l'information...
# On cherche plan de projection donc avec le maximum d'information possible pour le moindre coÃ»t...
# On veut que l'image projettÃ©e soit fidÃ¨le au modÃ¨le..
# =>Ici on est pret Ã  prendre 83% de l'information pour reprÃ©senter les composantes.
# On va y ajouter de nouvelles choses :
# On sait que la trace de la matrice = 6 (somme diago =6). Donc 1/6 * la somme des diago =1 (on a calculÃ© une moyenne).
# Donc on va mettre un seuil pour dÃ©finir d'accepter ou non une composante.
# On va donc conserver la valeur propre si elle est supÃ©rieur Ã  1.
# => Donc ici on va seulement conserver 2 valeurs propres (les deux premiÃ¨res). => C'est la mÃ©thode de KAISER.
# RQ : InterpretatbilitÃ© ... Quand on a 8/9.. On peut discuter d'accepter ou on la composante principale...
# Dans notre cas on va devoir accepter la troisiÃ¨me composante principale pour recupÃ©rer et obtenir 80% de l'information.
#Pour accÃ¨der aux variances
mydata.pca$var
contribution_moy_theo <- 100/nrow(mydata);
#Pour accÃ¨der aux ..
mydata.pca$ind
#Contributions
mydata.pca$ind$coord
# Pour avoir le tableau du poly page 52
mydata.pca$var$cos2
# On affiche le graphique de l'acp
plot.PCA(mydata.pca, axes=c(1,2),choix="var")
plot.PCA(mydata.pca, axes=c(3,1),choix="var")
plot.PCA(mydata.pca, axes=c(2,3),choix="var")
plot.PCA(mydata.pca, axes=c(1,2),choix="ind")
# Pour aller plus loins :
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, quanti.sup = 1)
plot.PCA(mydata.pcaN, axes=c(1,2),choix="var")
mydata.pcaN = PCA(mydata, scale.unit=TRUE, ncp=6, graph=FALSE, ind.sup = c(1,2))
plot.PCA(mydata.pcaN, axes=c(1,2),choix="ind")
# POUR AVOIR LE GRAPHIQUE DES VALEURS PROPRES :
fviz_eig(mydata.pca, addlabels = TRUE, ylim = c(0, 50))
#Coupure a dim=3 donc on garde 3 facteurs
# Avec le critÃ¨re que Kaiser, on garde 3 axes qui reprÃ©sente Ã  eux 83% de la variance
fviz_eig(mydata.pca,choice = "eigenvalue", addlabels = TRUE )+ geom_hline (yintercept = 1, linetype = 2, color= "red")
barplot(mydata.pca$eig[,3]) + lines(c(0,20),c(80,80),type = "l", lty=2,col="red")
#Realisation CAH
#install.packages("ggplot")
library(ggplot)
library(plyr)
library(philentropy)
library(factoextra)
#diviser en 4 clusters
data.hcpc <- HCPC(mydata.pca, nb.clust = 4, proba = 1, graph=FALSE)
#visualiser gain d'inertie
plot(data.hcpc, choice="bar")
#diminuer nombre de clisters à 3
data.hcpc <- HCPC(mydata.pca, nb.clust = 3, proba = 1, graph=FALSE)
#pour avoir les groupes/classes
plot(data.hcpc, choice="tree")
#analyser les vars les + explicites pour les classes
data.hcpc$desc.var$quanti$'1'
data.hcpc$desc.var$quanti$'2'
data.hcpc$desc.var$quanti$'3'
pl1 <- fviz_cluster(data.hcpc, ellipse = FALSE )
fviz_add(pl1, df=mydata, axes = c(1,2))
?fviz_add
?fviz_cluster
plot(data.hcpc,choice="map",draw.tree = F)
plot(data.hcpc,choice="map",draw.tree = F)
